# Proof_Of_Concept
Développer une preuve de concept
La segmentation sémantique occupe aujourd’hui une place très importante dans le domaine de la vision par ordinateur ou computer vision en anglais. Elle permet de classifier des images en attribuant une étiquette de classe à chaque pixel. Son objectif est de regrouper les pixels représentant des objets similaires et de leur accorder une catégorie prédéfinie. Elle trouve son application dans divers domaines dont la voiture autonome, la surveillance vidéo intelligente et les diagnostics médicaux précis. Le récent essor des réseaux de neurones convolutifs (CNN) a permis de réelles avancées dans le domaine de la segmentation sémantique d’images. Des modèles comme U-Net ou VGG16 ont permis d’atteindre des performances sans précédent. Mais les CNN, bien que performants, présentent plusieurs faiblesses dont la dépendance aux données massives, leur incapacité à capturer le contexte global et un besoin important en termes de puissance de calcul. C’est dans ce cadre que les Transformers ont fait leur entrée dans ce domaine. En effet, leur capacité de modélisation des dépendances de longue portée et leur légèreté relativement aux CNN ont fait d’eux un outil de choix dans les tâches de segmentation sémantique.
Dans ce papier, nous proposons une étude comparative entre un modèle U-Net Mini qui nous sert de baseline et un SegFormer qui est considéré comme le state of the art des modèles de segmentation sémantique. Le SegFormer est un modèle qui s’appuie sur l’architecture Transformers.
 Pour les besoins de cette étude, nous utilisons les modèles pré entrainés « Nvidia/segformer-b0-finetuned-cityscapes-1024-1024 » pré entrainé sur les données Cityscapes et le « Nvidia/segformer-b0-finetuned-ade-512-512 » pré entrainé sur les données ADE20K.
